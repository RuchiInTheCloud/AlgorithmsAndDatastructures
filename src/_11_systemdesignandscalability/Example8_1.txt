Paste bin: Enter piece of text and randomly generated URL for public access

Scope the problem
- System does not support user accounts or editing documents
- System tracks analytics of how often a page is accessed
- Old documents get deleted after not being accessed for a long time
- No authentication to access documents, not possible to guess URL easily
- System has frontend as well as API
- Analytics of each URL can be accessed through stats link on each page. Not shown by default

Make reasonable assumptions
- System gets heavy traffic
- Contains millions of documents
- Traffic not equally distributed across documents. Some accessed more frequently than others.

Draw major components
- Track URL and related file, Analytics of a URL
- Store documents in file or database? Since search capability not needed store in file
- Database (URL, Location(Server + File path))
- Analytics: Add each visit to data store (timestamp, IP, location)
-- To get stats pull information from database

Identify the key issues
- Some documents accessed more frequently than others.
-- Reading data from file system relatively slow compared to reading data from memory
-- Use cache to store most recently/ frequently accessed documents
-- Since documents cannot be edited no need to invalidate cache
-- Sharding a database. hash(URL)%int, locate database containing the file
-- Skip database. Let hash determine the machine. If we need to add servers difficult to redistribute documents.
- Generating URLs
-- Generate random guid: 128 bit value with low odds of collision
-- If we hash to a smaller value, there is risk of collision
-- Alternatively generate a 10 charachter sequence of letters and numbers. 36^10 possible strings
--- Odds of collision low. Collision may occur after storing 1 billion URLs
--- Check if URL exists, if URL maps to server, if document exists at the server, if yes regenerate URL
- Analytics
-- Display number of visits (by location and time)
-- Two options:
--- Store data from each visit
--- Store data we know we will use (number of visits by location, time, URL)
- Raw data gives flexibility
- Store visits in files conditionally based on storage_probability since space usage would be enormous
-- As popularity goes up, storage probability goes down
-- Log popular document, one out of every 10 times at random. When looking at number of visits for the site multiply by 10
-- Log file not used frequently. Store precomputed data in data store
-- URL, month, visits
-- everytime URL visited increment counter
-- datastore can be sharded by URL
-- Stats not frequently visited, no heavy load
-- stats of popular websites, the generated html could be cached on frontend servers to avoid reaccess for the most popular URLs

Follow up questions
- How would you support user accounts
- How would you add a new piece of analytics (referal source) to the stats page?
- How would your design change if stats were shown with each document?